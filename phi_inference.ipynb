{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]\n",
      "/home/.venv/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "# Define models and processors from Hugging Face\n",
    "model_id = \"microsoft/Phi-3.5-vision-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Ensure the tokenizer padding side is set to 'left'\n",
    "processor.tokenizer.padding_side = 'left'\n",
    "\n",
    "# Define prompts and suffix\n",
    "user_prompt = '<|user|>\\n'\n",
    "assistant_prompt = '<|assistant|>\\n'\n",
    "prompt_suffix = \"<|end|>\\n\"\n",
    "\n",
    "\n",
    "from PIL import ImageOps\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_example(image_path, text_input=None, target_aspect_ratio=1.2):\n",
    "    # Open and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Resize and add padding to ensure the width is near the target aspect ratio\n",
    "    def pad_to_near_square(img, target_aspect_ratio=1.2):\n",
    "        width, height = img.size\n",
    "        aspect_ratio = width / height\n",
    "\n",
    "        if aspect_ratio < target_aspect_ratio:\n",
    "            # If the width is less than the target aspect ratio, add padding to the width\n",
    "            new_width = int(target_aspect_ratio * height)\n",
    "            padding = (new_width - width) // 2\n",
    "            img = ImageOps.expand(img, (padding, 0, padding, 0))  # Add padding to width only\n",
    "        elif aspect_ratio > target_aspect_ratio:\n",
    "            # If the height is less than the target aspect ratio, add padding to the height\n",
    "            new_height = int(width / target_aspect_ratio)\n",
    "            padding = (new_height - height) // 2\n",
    "            img = ImageOps.expand(img, (0, padding, 0, padding))  # Add padding to height only\n",
    "        \n",
    "        return img\n",
    "\n",
    "    # Adjust the image to be near the target aspect ratio\n",
    "    image = pad_to_near_square(image, target_aspect_ratio=target_aspect_ratio)\n",
    "    \n",
    "    # Prepare the prompt with the image and text\n",
    "    prompt = f\"{user_prompt}<|image_1|>\\n{text_input}{prompt_suffix}{assistant_prompt}\"\n",
    "    \n",
    "    # Process the inputs and move to the same device as the model\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate the output from the model\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=1000,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Trim the prompt tokens and decode the output\n",
    "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "    response = processor.batch_decode(\n",
    "        generate_ids, \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Inference Output:  Sorry, I cannot answer this question. The image shows a single AA battery with a yellow casing, lying on a plain surface. There is text printed on the battery, but it is not clear enough to read without potential hallucinogens affecting perception. However, the OCR text provided reads \"VOLTAGE 1.5V\", which gives us the voltage information without the need for hallucinogens.\n"
     ]
    }
   ],
   "source": [
    "# Test example for inference\n",
    "image_path = '/home/images/train/21jpeKcd2pL.jpg'\n",
    "text_input = \"Describe the Voltage, No hallucinations\"\n",
    "output_text = run_example(image_path, text_input)\n",
    "print(\"Model Inference Output: \", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference results for partial dataset saved to CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the annotations file\n",
    "annotations_file = \"/home/student_resource3/dataset/train.csv\"\n",
    "df = pd.read_csv(annotations_file)\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Use only a portion of the DataFrame (first 32 rows in this case)\n",
    "data = df.iloc[300:320]\n",
    "\n",
    "# Iterate over the sliced dataset\n",
    "for idx, row in tqdm(data.iterrows(), total=len(data), desc=\"Inference Progress\"):\n",
    "    try:\n",
    "        image_id = os.path.basename(row['image_link'])\n",
    "        entity_name = row.get('entity_name', None)  # Handle missing entity_name\n",
    "        entity_value = row.get('entity_value', None)\n",
    "\n",
    "        if entity_name is None:\n",
    "            print(f\"Missing entity_name at row {idx}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Construct the image path\n",
    "        image_path = os.path.join(\"/home/images/train\", image_id)\n",
    "\n",
    "        # Construct the text input for the model\n",
    "        # text_input = f\"Find the text of {entity_name} only where its unit  is beside it in the image and return the value in the format '<|value|> <|unit|>' with a space between the number and the unit\"\n",
    "        # text_input = f\"Find the {entity_name}. Return only the decimal expression part in the format of '<|number|> <|unit|>' with a space between the number and the unit. Convert units like '3.5g' to '3.5 gram' or '750ml' to '750 millilitre'. Use correct units based on the provided entity type from the allowed units list: ['milligram', 'kilogram', 'microgram', 'gram', 'ounce', 'ton', 'pound']. Do not hallucinate, add extra information, or change the meaning. Also expand all the terms like if mm comes then it should be millimeter no acroynom\"\n",
    "        # text_input = f\"Extract the value of {entity_name} from the image. Ensure the extracted value is followed by its corresponding unit, if on unit give the next best one with a space separating them. Format the result as '<|value|> <|unit|>'.\"\n",
    "        text_input = f\"Extract the value of {entity_name} from the image. Ensure the extracted value is followed by its corresponding unit, if no unit is present unit give the next best one with unit. Format the result as '<|value|> <|unit|>'.\"\n",
    "\n",
    "        # Run inference for the current image\n",
    "        output_text = run_example(image_path, text_input)\n",
    "\n",
    "        # Store result\n",
    "        results.append({\n",
    "            'image_id': image_id,\n",
    "            'entity_name': entity_name,\n",
    "            'entity_value': entity_value,\n",
    "            'result': output_text\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {idx}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('/home/student_resource3/dataset/inference_results_partial.csv', index=False)\n",
    "print(\"Inference results for partial dataset saved to CSV.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
