{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW,AutoModelForCausalLM, AutoProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageOps,Image\n",
    "import cv2\n",
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "/home/.venv/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"microsoft/Phi-3.5-vision-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "processor.tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "user_prompt = '<|user|>\\n'\n",
    "assistant_prompt = '<|assistant|>\\n'\n",
    "prompt_suffix = \"<|end|>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def run_example(image, text_input=None, target_aspect_ratio=1.2):\n",
    "    \n",
    "\n",
    "    def pad_to_near_square(img, target_aspect_ratio=1.2):\n",
    "        width, height = img.size\n",
    "        aspect_ratio = width / height\n",
    "\n",
    "        if aspect_ratio < target_aspect_ratio:\n",
    "\n",
    "            new_width = int(target_aspect_ratio * height)\n",
    "            padding = (new_width - width) // 2\n",
    "            img = ImageOps.expand(img, (padding, 0, padding, 0))\n",
    "        elif aspect_ratio > target_aspect_ratio:\n",
    "\n",
    "            new_height = int(width / target_aspect_ratio)\n",
    "            padding = (new_height - height) // 2\n",
    "            img = ImageOps.expand(img, (0, padding, 0, padding)) \n",
    "        \n",
    "        return img\n",
    "\n",
    "    image = pad_to_near_square(image, target_aspect_ratio=target_aspect_ratio)\n",
    "    \n",
    "\n",
    "    prompt = f\"{user_prompt}<|image_1|>\\n{text_input}{prompt_suffix}{assistant_prompt}\"\n",
    "    \n",
    "\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=1000,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "    response = processor.batch_decode(\n",
    "        generate_ids, \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def split_image_into_quarters(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image at path {image_path} could not be loaded.\")\n",
    "    \n",
    "    # Get image dimensions\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Calculate sizes for each grid section\n",
    "    third_width = width // 3\n",
    "    third_height = height // 3\n",
    "    \n",
    "    # Define the regions for each of the 9 parts\n",
    "    top_left = image[:third_height, :third_width]\n",
    "    top_center = image[:third_height, third_width:2*third_width]\n",
    "    top_right = image[:third_height, 2*third_width:]\n",
    "    \n",
    "    middle_left = image[third_height:2*third_height, :third_width]\n",
    "    middle_center = image[third_height:2*third_height, third_width:2*third_width]\n",
    "    middle_right = image[third_height:2*third_height, 2*third_width:]\n",
    "    \n",
    "    bottom_left = image[2*third_height:, :third_width]\n",
    "    bottom_center = image[2*third_height:, third_width:2*third_width]\n",
    "    bottom_right = image[2*third_height:, 2*third_width:]\n",
    "    \n",
    "    return [top_left, top_center, top_right, middle_left, middle_center, middle_right, bottom_left, bottom_center, bottom_right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 100/100 [02:52<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference results for partial dataset saved to CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the test annotations file\n",
    "annotations_file = \"/home/student_resource 3/dataset/test.csv\"\n",
    "df = pd.read_csv(annotations_file)\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "data = df.iloc[0:100]\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in tqdm(data.iterrows(), total=len(data), desc=\"Inference Progress\"):\n",
    "    try:\n",
    "        image_id = os.path.basename(row['image_link'])\n",
    "        entity_name = row.get('entity_name', None) \n",
    "        entity_id = row.get('index', 0)\n",
    "\n",
    "        if entity_name is None:\n",
    "            print(f\"Missing entity_name at row {idx}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(\"/home/images/test\", image_id)\n",
    "\n",
    "        # Construct the text input for the Phi model\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        text_input = f\"Extract the value of {entity_name} from the image. Ensure the extracted value is followed by its corresponding unit, if no unit is present give the next best one. Format the result as '<|value|> <|unit|>'.\"\n",
    "\n",
    "        # Run inference using the Phi model\n",
    "        final_output = run_example(image, text_input)\n",
    "\n",
    "        if not bool(re.search(r'\\d', final_output)):\n",
    "            quarts = split_image_into_quarters(image_path)\n",
    "            for i in quarts:\n",
    "                final_output = run_example(Image.fromarray(i), text_input)\n",
    "                if bool(re.search(r'\\d', final_output)):\n",
    "                    break\n",
    "            final_output=\"\"\n",
    "            \n",
    "\n",
    "        # Append results\n",
    "        results.append({\n",
    "            'index': entity_id,\n",
    "            'prediction': final_output\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {idx}: {e}\")\n",
    "\n",
    "        # Append results even if an error occurs\n",
    "        results.append({\n",
    "            'index': entity_id,\n",
    "            'prediction': \"\"\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('/home/inference0to10k22.csv', index=False)\n",
    "print(\"Inference results for partial dataset saved to CSV.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
